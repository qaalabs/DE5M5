{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#the-data-engineering-product-lifecycle","title":"The Data Engineering Product Lifecycle","text":""},{"location":"#module-4-planning-a-data-engineering-product","title":"Module 4: Planning a Data Engineering Product","text":"<p>What problem are we solving and how will we design the solution?</p> <p>Defining business needs, architecting data solutions, and creating comprehensive blueprints for success.</p>"},{"location":"#module-5-data-engineering-product-development","title":"Module 5: Data Engineering Product Development","text":"<p>How do we bring our design to life?</p> <p>Transforming plans into functional data pipelines, implementing storage solutions, and creating robust processing workflows.</p>"},{"location":"#module-6-data-operations","title":"Module 6: Data Operations","text":"<p>How do we ensure lasting value?</p> <p>Deploying, monitoring, and evolving data products to deliver continuous business impact through changing requirements.</p>"},{"location":"404/","title":"Page Not Found","text":""},{"location":"404/#page-not-found","title":"Page Not Found","text":"<p>Sorry, the page you're looking for does not exist.</p> <ul> <li>Check the URL for typos</li> <li>Go back to the home page</li> </ul>"},{"location":"resources/","title":"Resources","text":""},{"location":"resources/#resources","title":"Resources","text":""},{"location":"resources/#github","title":"GitHub","text":"<ul> <li>Library Pipeline Template</li> </ul>"},{"location":"resources/#links","title":"Links","text":"<ul> <li>Learner reactions ~ Ticks &amp; Crosses</li> </ul>"},{"location":"resources/#drawing-tools","title":"Drawing tools","text":"<ul> <li>Microsoft Visio (on the VM)</li> <li>https://app.diagrams.net/ ~ was draw.io</li> <li>https://dbdiagram.io/home</li> <li>https://online.visual-paradigm.com/</li> </ul>"},{"location":"trainer/","title":"Trainer Notes","text":""},{"location":"trainer/#trainer-notes","title":"Trainer Notes","text":""},{"location":"trainer/#day-1-foundation-setup","title":"Day 1 - Foundation &amp; Setup","text":""},{"location":"trainer/#session-1","title":"Session 1","text":"<ul> <li>The Scenario</li> <li>Your Mission</li> <li>Discussion: What does 'Production-Ready' mean?</li> <li>Review the sample data files</li> </ul>"},{"location":"trainer/#session-2","title":"Session 2","text":"<ul> <li>Architecture Principles</li> <li>Demo: Creating Architecture Diagrams</li> <li>Activity: Design your pipeline architecture</li> <li>Review the diagrams created</li> </ul>"},{"location":"trainer/#session-3","title":"Session 3","text":"<ul> <li>Demo: GitHub Template Setup</li> <li>Activity: Create a GitHub repository</li> <li>Activity: Introduce GitHub Projects</li> <li>Activity: Set up Kanban board</li> <li>Review the GitHub repositories</li> </ul>"},{"location":"trainer/#session-4","title":"Session 4","text":"<ul> <li>Local development</li> <li>Create local development env</li> <li>Activity: Write first Python code</li> <li>Write first Pull Request</li> </ul>"},{"location":"trainer/#day-2-build-test-pyhon-package","title":"Day 2 - Build &amp; Test Pyhon Package","text":""},{"location":"trainer/#session-1_1","title":"Session 1","text":"<ul> <li>Demo: Explain function - Data Ingestion</li> <li>Activity: Write function - Data Ingestion</li> </ul>"},{"location":"trainer/#session-2_1","title":"Session 2","text":"<ul> <li>Demo: Explain function - Data Cleaning</li> <li>Activity: Write function - Data Cleaning</li> </ul>"},{"location":"trainer/#session-3_1","title":"Session 3","text":"<ul> <li>Demo: Python Testing</li> <li>Activity: Write unit tests</li> <li>Activity: Run tests &amp; check coverage</li> </ul>"},{"location":"trainer/#session-4_1","title":"Session 4","text":"<ul> <li>Demo: Python Test Coverage</li> <li>Activity: Achive test coverage</li> <li>Activity: Commit &amp; Cleanup VM</li> </ul>"},{"location":"trainer/#day-3-automation-integration","title":"Day 3 - Automation &amp; Integration","text":""},{"location":"trainer/#session-1_2","title":"Session 1","text":""},{"location":"trainer/#session-2_2","title":"Session 2","text":""},{"location":"trainer/#session-3_2","title":"Session 3","text":""},{"location":"trainer/#session-4_2","title":"Session 4","text":""},{"location":"trainer/#day-4-polish-present","title":"Day 4 - Polish &amp; Present","text":""},{"location":"trainer/#session-1_3","title":"Session 1","text":""},{"location":"trainer/#session-2_3","title":"Session 2","text":""},{"location":"trainer/#session-3_3","title":"Session 3","text":""},{"location":"trainer/#session-4_3","title":"Session 4","text":""},{"location":"agenda/day1/","title":"Day 1","text":""},{"location":"agenda/day1/#day-1-foundation-setup","title":"Day 1 - Foundation &amp; Setup","text":""},{"location":"agenda/day1/#session-1","title":"Session 1","text":"<ul> <li><code>09:30</code> VM Setup (10 mins)</li> <li><code>09:40</code> Welcome (10 mins)</li> <li><code>09:50</code> The Scenario (10 mins)</li> <li><code>10:00</code> Your Mission (10 mins)</li> <li><code>10:10</code> Discussion: What does 'Production-Ready' mean? (10 mins)</li> <li><code>10:20</code> Review the sample data files (20 mins)</li> </ul>"},{"location":"agenda/day1/#morning-break","title":"\u2615 Morning Break","text":""},{"location":"agenda/day1/#session-2","title":"Session 2","text":"<ul> <li><code>11:00</code> Architecture Principles (20 mins)</li> <li><code>11:20</code> Demo: Creating Architecture Diagrams (10 mins)</li> <li><code>11:30</code> Activity: Design your pipeline architecture (40 mins)</li> <li><code>12:10</code> Review the diagrams created (20 mins)</li> </ul>"},{"location":"agenda/day1/#lunch-break","title":"\ud83e\udd6a\ud83e\udd64 Lunch Break","text":""},{"location":"agenda/day1/#session-3","title":"Session 3","text":"<ul> <li><code>13:30</code> Demo: GitHub Template Setup (20 mins)</li> <li><code>13:50</code> Activity: Create a GitHub repository (10 mins)</li> <li><code>14:00</code> Activity: Introduce GitHub Projects (10 mins)</li> <li><code>14:10</code> Activity: Set up Kanban board (10 mins)</li> <li><code>14:20</code> Review the GitHub repositories (10 mins)</li> </ul>"},{"location":"agenda/day1/#afternoon-break","title":"\u2615 Afternoon Break","text":""},{"location":"agenda/day1/#session-4","title":"Session 4","text":"<ul> <li><code>14:50</code> Local development (20 mins)</li> <li><code>15:10</code> Create local development env (10 mins)</li> <li><code>15:20</code> Activity: Write first Python code (20 mins)</li> <li><code>15:40</code> Write first Pull Request (10 mins)</li> </ul>"},{"location":"agenda/day1/#wrap","title":"Wrap","text":""},{"location":"agenda/day2/","title":"Day 2","text":""},{"location":"agenda/day2/#day-2-build-test-pyhon-package","title":"Day 2 - Build &amp; Test Pyhon Package","text":""},{"location":"agenda/day2/#session-1","title":"Session 1","text":"<ul> <li><code>09:30</code> Welcome (10 mins)</li> <li><code>09:40</code> VM Setup &amp; Welcome (10 mins)</li> <li><code>09:50</code> Demo: Explain function - Data Ingestion (20 mins)</li> <li><code>10:10</code> Activity: Write function - Data Ingestion (30 mins)</li> </ul>"},{"location":"agenda/day2/#morning-break","title":"\u2615 Morning Break","text":""},{"location":"agenda/day2/#session-2","title":"Session 2","text":"<ul> <li><code>11:00</code> Demo: Explain function - Data Cleaning (20 mins)</li> <li><code>11:20</code> Activity: Write function - Data Cleaning (60 mins)</li> </ul>"},{"location":"agenda/day2/#lunch-break","title":"\ud83e\udd6a\ud83e\udd64 Lunch Break","text":""},{"location":"agenda/day2/#session-3","title":"Session 3","text":"<ul> <li><code>13:20</code> Demo: Python Testing (30 mins)</li> <li><code>13:50</code> Activity: Write unit tests (40 mins)</li> <li><code>14:30</code> Activity: Run tests &amp; check coverage (10 mins)</li> </ul>"},{"location":"agenda/day2/#afternoon-break","title":"\u2615 Afternoon Break","text":""},{"location":"agenda/day2/#session-4","title":"Session 4","text":"<ul> <li><code>15:00</code> Demo: Python Test Coverage (10 mins)</li> <li><code>15:10</code> Activity: Achive test coverage (30 mins)</li> <li><code>15:40</code> Activity: Commit &amp; Cleanup VM (10 mins)</li> </ul>"},{"location":"agenda/day2/#wrap","title":"Wrap","text":""},{"location":"agenda/day3/","title":"Day 3 - Automation &amp; Integration","text":""},{"location":"agenda/day3/#day-3-automation-integration","title":"Day 3 - Automation &amp; Integration","text":""},{"location":"agenda/day3/#session-1","title":"Session 1","text":"<ul> <li><code>09:30</code> Welcome (10 mins)</li> </ul>"},{"location":"agenda/day3/#morning-break","title":"\u2615 Morning Break","text":""},{"location":"agenda/day3/#session-2","title":"Session 2","text":""},{"location":"agenda/day3/#lunch-break","title":"\ud83e\udd6a\ud83e\udd64 Lunch Break","text":""},{"location":"agenda/day3/#session-3","title":"Session 3","text":""},{"location":"agenda/day3/#afternoon-break","title":"\u2615 Afternoon Break","text":""},{"location":"agenda/day3/#session-4","title":"Session 4","text":""},{"location":"agenda/day3/#wrap","title":"Wrap","text":""},{"location":"agenda/day4/","title":"Day 4 - Polish &amp; Present","text":""},{"location":"agenda/day4/#day-4-polish-present","title":"Day 4 - Polish &amp; Present","text":""},{"location":"agenda/day4/#session-1","title":"Session 1","text":"<ul> <li><code>09:30</code> Welcome (10 mins)</li> </ul>"},{"location":"agenda/day4/#morning-break","title":"\u2615 Morning Break","text":""},{"location":"agenda/day4/#session-2","title":"Session 2","text":""},{"location":"agenda/day4/#lunch-break","title":"\ud83e\udd6a\ud83e\udd64 Lunch Break","text":""},{"location":"agenda/day4/#session-3","title":"Session 3","text":""},{"location":"agenda/day4/#afternoon-break","title":"\u2615 Afternoon Break","text":""},{"location":"agenda/day4/#session-4","title":"Session 4","text":""},{"location":"agenda/day4/#wrap","title":"Wrap","text":""},{"location":"day1/create-diagram/","title":"Architecture Design &amp; Documentation","text":""},{"location":"day1/create-diagram/#architecture-design-documentation","title":"Architecture Design &amp; Documentation","text":""},{"location":"day1/create-diagram/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Design a medallion architecture for the project</li> <li>Create clear architecture diagrams</li> <li>Document key decisions</li> </ul>"},{"location":"day1/create-diagram/#architecture-principles","title":"Architecture Principles","text":"<p>Medallion pattern: Bronze \u2192 Silver \u2192 Gold</p> <p>Why this pattern? (separation of concerns, reprocessing)</p> <p>Where does each transformation happen?</p> <p>Visual:</p> <pre><code>Sources (CSV, JSON, Excel)\n     \u2193\nBRONZE (Raw ingestion)\n     \u2193\nSILVER (Cleaned, validated) \u2190 Your Python package does this\n     \u2193\nGOLD (Business aggregations)\n     \u2193\nConsumption (Reports, analysis)\n</code></pre>"},{"location":"day1/create-diagram/#creating-architecture-diagrams","title":"Creating Architecture Diagrams","text":"<ul> <li>Use draw.io (in browser, free)</li> <li>What to include: sources, layers, technologies</li> <li>Keep it simple!</li> </ul>"},{"location":"day1/create-diagram/#live-demo-create-a-simple-diagram","title":"Live demo: Create a simple diagram","text":""},{"location":"day1/create-project/","title":"Introduce GitHub Projects","text":""},{"location":"day1/create-project/#create-a-project-in-github","title":"Create a project in GitHub","text":"<p>Go to YOUR repository on GitHub</p>"},{"location":"day1/create-project/#step-1-create-project-screen-share","title":"Step 1: Create Project (Screen share)","text":"<ul> <li>Go to your repo (that you create from the template)</li> <li>Click \"Projects\" tab</li> <li>Click \"New project\"</li> <li>Enter name: \"Library Pipeline Development\"</li> <li>Choose \"Board\" template</li> <li>Click \"Create\"</li> </ul>"},{"location":"day1/create-project/#step-2-customize-columns","title":"Step 2: Customize Columns","text":"<p>Rename columns to: - <code>To Do</code> - <code>In Progress</code> - <code>Testing</code> - <code>Done</code></p> <p>GitHub may auto-create these</p>"},{"location":"day1/create-project/#step-3-add-tasks","title":"Step 3: Add Tasks","text":"<ul> <li>Click \"+ Add item\" in To Do column</li> <li>Type: \"Build CSV ingestion function\"</li> <li>Press Enter</li> <li>Repeat for a few more tasks</li> </ul>"},{"location":"day1/create-project/#step-4-show-movement","title":"Step 4: Show Movement","text":"<ul> <li>Drag \"Setup GitHub repo\" to Done</li> <li>See how it updates</li> </ul>"},{"location":"day1/create-project/#step-5-link-to-code","title":"Step 5: Link to Code","text":"<ul> <li>Click on a task</li> <li>Add description: \"Implement load_csv() in ingestion.py\"</li> <li>Save</li> </ul>"},{"location":"day1/create-repo/","title":"Create a GitHub repository","text":""},{"location":"day1/create-repo/#activity-create-repository","title":"Activity: Create Repository","text":""},{"location":"day1/create-repo/#create-repo-from-a-template","title":"Create repo from a template","text":"<p>Go to template URL: - https://github.com/ingwanelabs/library-pipeline-template</p> <p>\"Use this template\" \u2192 Create repo</p> <p>Name it: <code>library-pipeline</code></p>"},{"location":"day1/create-repo/#clone-to-vm","title":"Clone to VM","text":"<ul> <li>Open terminal</li> <li>Clone repository</li> <li>Navigate into folder</li> <li>Explore the structure</li> </ul>"},{"location":"day1/create-repo/#environment-setup","title":"Environment Setup","text":"<ul> <li>Create virtual environment</li> <li>Install dependencies</li> <li>Run example test</li> <li>Verify it works</li> </ul>"},{"location":"day1/create-repo/#first-commit","title":"First Commit","text":"<ul> <li>Upload architecture diagram to docs/architecture/</li> <li>Commit and push</li> <li>Verify on GitHub</li> </ul>"},{"location":"day1/design-pipeline/","title":"Design your pipeline architecture","text":""},{"location":"day1/design-pipeline/#activity-design-your-pipeline-architecture","title":"Activity: Design your Pipeline Architecture","text":"<ul> <li>Open draw.io or Excalidraw</li> <li>Design your pipeline architecture</li> </ul>"},{"location":"day1/design-pipeline/#include","title":"Include:","text":"<ul> <li>Data sources</li> <li>Bronze/Silver/Gold layers</li> <li>Technologies (Python, Pandas, Fabric, GitHub Actions)</li> <li>Data flow arrows</li> </ul> <p>Save as PNG or PDF</p> <p>Keep it simple! Don't over-engineer</p>"},{"location":"day1/github-template/","title":"GitHub Template Setup","text":""},{"location":"day1/github-template/#github-template-setup","title":"GitHub Template Setup","text":""},{"location":"day1/github-template/#learning-objectives","title":"Learning Objectives:","text":"<ul> <li>Use GitHub template to create project repository</li> <li>Clone to VM and set up local environment</li> <li>Understand the project structure</li> </ul>"},{"location":"day1/github-template/#github-template-introduction","title":"GitHub Template Introduction","text":"<ul> <li>What is a template repository?</li> <li>Why use one? (structure, best practices, working CI/CD)</li> </ul>"},{"location":"day1/github-template/#live-demo-create-from-template-10-mins","title":"Live Demo: Create from Template (10 mins)","text":"<ul> <li>Navigate to template repo</li> <li>Click \"Use this template\"</li> <li>Create repository: library-pipeline</li> <li>Show the structure</li> <li>Explain what each folder is for</li> </ul>"},{"location":"day1/github-template/#clone-to-vm-setup-10-mins","title":"Clone to VM &amp; Setup (10 mins)","text":"<pre><code># Clone this repository\ngit clone https://github.com/YOUR_USERNAME/YOUR_REPO.git\ncd YOUR_REPO\n\n# Create virtual environment\npython -m venv venv\n.\\venv\\Scripts\\Activate.ps1\n\n# Confirm the Python 3 version\npython --version\n\n# Install dependencies\npip install -r requirements.txt\n\n# Run tests\npytest tests/ -v\n\n# Run Python tests with a coverage report\npytest tests/ -v --cov=src --cov-report=term-missing\n</code></pre>"},{"location":"day1/project-tasks/","title":"Set up Kanban board","text":""},{"location":"day1/project-tasks/#recommended-tasks-for-library-pipeline-project","title":"Recommended Tasks for Library Pipeline Project","text":""},{"location":"day1/project-tasks/#data-ingestion","title":"Data Ingestion","text":"<ul> <li> Build CSV ingestion function (<code>load_csv()</code>)</li> <li> Build JSON ingestion function (<code>load_json()</code>)</li> <li> Build Excel ingestion function (<code>load_excel()</code>)</li> <li> Build text file reader (<code>load_text()</code>)</li> <li> Test ingestion locally in Jupyter</li> </ul>"},{"location":"day1/project-tasks/#data-cleaning","title":"Data Cleaning","text":"<ul> <li> Implement duplicate removal function</li> <li> Implement missing value handler</li> <li> Implement date standardization</li> <li> Implement ISBN validation</li> <li> Test cleaning functions</li> </ul>"},{"location":"day1/project-tasks/#testing","title":"Testing","text":"<ul> <li> Write tests for ingestion module (70%+ coverage)</li> <li> Write tests for cleaning module (70%+ coverage)</li> <li> Write tests for validation module (70%+ coverage)</li> <li> Ensure all tests pass</li> </ul>"},{"location":"day1/project-tasks/#cicd","title":"CI/CD","text":"<ul> <li> Review GitHub Actions workflow</li> <li> Test workflow on feature branch</li> <li> Fix any failing CI/CD checks</li> <li> Implement pull request workflow</li> </ul>"},{"location":"day1/project-tasks/#deployment","title":"Deployment","text":"<ul> <li> Prepare data for Fabric upload</li> <li> Create Fabric notebook</li> <li> Deploy package to Fabric</li> <li> Test in Fabric environment</li> </ul>"},{"location":"day1/project-tasks/#documentation-presentation","title":"Documentation &amp; Presentation","text":"<ul> <li> Complete README</li> <li> Document architecture decisions</li> <li> Create presentation slides</li> <li> Prepare demo environment</li> <li> Practice presentation</li> </ul>"},{"location":"day1/sample-data/","title":"Review the sample data files","text":""},{"location":"day1/sample-data/#library-pipeline-sample-data","title":"Library Pipeline: Sample Data","text":""},{"location":"day1/sample-data/#sample-data","title":"Sample Data","text":"<p>This folder contains sample data for the library pipeline project.</p> <ul> <li>GitHub Link: https://github.com/ingwanelabs/library-pipeline-template/tree/main/data</li> </ul>"},{"location":"day1/sample-data/#data-files","title":"Data Files","text":""},{"location":"day1/sample-data/#circulation_datacsv-50000-rows","title":"<code>circulation_data.csv</code> (50,000 rows)","text":"<ul> <li>Transaction records from book checkouts</li> <li>Contains quality issues: duplicates, missing values, date format inconsistencies</li> </ul>"},{"location":"day1/sample-data/#events_datajson-500-events","title":"<code>events_data.json</code> (500 events)","text":"<ul> <li>Library events and attendance records</li> <li>Nested JSON structure requiring flattening</li> </ul>"},{"location":"day1/sample-data/#feedbacktxt-200-feedback-entries","title":"<code>feedback.txt</code> (200 feedback entries)","text":"<ul> <li>Unstructured member feedback</li> <li>Text parsing required</li> </ul>"},{"location":"day1/sample-data/#cataloguexlsx-5000-books","title":"<code>catalogue.xlsx</code> (5,000 books)","text":"<ul> <li>Book catalogue with acquisition data</li> <li>Multiple sheets, type inconsistencies</li> </ul>"},{"location":"day1/sample-data/#data-quality-issues","title":"Data Quality Issues","text":"<p>These files intentionally contain data quality issues:</p> <ul> <li>Duplicate records</li> <li>Missing values</li> <li>Inconsistent date formats</li> <li>Nested structures requiring flattening</li> </ul> <p>Your task is to build a pipeline that cleans and validates this data.</p>"},{"location":"day1/scenario/","title":"The Scenario","text":""},{"location":"day1/scenario/#the-scenario","title":"The Scenario","text":"<p>Newham Public Library Network manages 15 branches across East London. </p> <p>They collect data from multiple sources:</p>"},{"location":"day1/scenario/#circulation-system-csv-exports","title":"Circulation system (CSV exports):","text":"<ul> <li>Book loans</li> <li>Returns</li> <li>Reservations</li> </ul>"},{"location":"day1/scenario/#event-management-system-json-api","title":"Event management system (JSON API):","text":"<ul> <li>Community events</li> <li>Attendance</li> </ul>"},{"location":"day1/scenario/#member-feedback-unstructured-text-files","title":"Member feedback (unstructured text files):","text":"<ul> <li>Survey responses</li> <li>Complaints</li> </ul>"},{"location":"day1/scenario/#digital-catalogue-excel-files","title":"Digital catalogue (Excel files)","text":"<ul> <li>Book metadata</li> <li>Acquisition records</li> </ul>"},{"location":"day1/scenario/#current-problems","title":"Current Problems:","text":"<ul> <li>Data quality issues: duplicates, missing values, inconsistent formats</li> <li>Manual consolidation takes 2-3 days per month</li> <li>No automated quality checks</li> </ul>"},{"location":"day1/scenario/#difficult-to-answer-questions-like","title":"Difficult to answer questions like:","text":"<ul> <li>Which books are most popular by borough?</li> <li>What's our member retention rate?</li> <li>Which events drive library visits?</li> </ul> <p>Data analysts spend 60% of time cleaning, 40% analysing</p>"},{"location":"day1/scenario/#librarys-request","title":"Library's Request","text":"<p>They want an automated data quality pipeline that:</p> <ul> <li>Ingests data from multiple sources</li> <li>Cleans and validates data automatically</li> <li>Produces analysis-ready datasets</li> <li>Runs on a schedule</li> <li>Has quality monitoring and alerts</li> </ul> <p>They've heard that modern data engineering practices and CI/CD can help but don't know where to start.</p>"},{"location":"day1/scenario/#your-task","title":"Your Task","text":"<p>Design and build a production-ready data quality pipeline that solves the library's problem.</p>"},{"location":"day1/scenario/#phase-1-planning-documentation-day-1","title":"Phase 1: Planning &amp; Documentation (Day 1)","text":"<ol> <li> <p>Analyse requirements and identify data quality issues</p> </li> <li> <p>Design system architecture using medallion pattern:</p> <ul> <li>Bronze: Raw data ingestion</li> <li>Silver: Cleaned and validated data</li> <li>Gold: Analysis-ready aggregations</li> </ul> </li> <li> <p>Create documentation:</p> <ul> <li>Architecture diagrams</li> <li>Data flow documentation</li> <li>ADRs (Architecture Decision Records)</li> </ul> </li> <li> <p>Set up Kanban board for sprint planning</p> <ul> <li>Initialise GitHub repository</li> </ul> </li> </ol>"},{"location":"day1/scenario/#phase-2-development-day-2","title":"Phase 2: Development (Day 2)","text":"<ol> <li> <p>Build Python package for data processing:</p> <ul> <li>Extract data from CSV, JSON, Excel sources (S16)</li> <li>Clean data using Pandas</li> <li>Implement validation rules</li> <li>Handle errors gracefully</li> </ul> </li> <li> <p>Write unit tests for all functions</p> </li> <li> <p>Document your code</p> </li> </ol>"},{"location":"day1/scenario/#phase-3-pipeline-automation-day-3","title":"Phase 3: Pipeline &amp; Automation (Day 3)","text":"<ol> <li> <p>Build data pipeline in Microsoft Fabric:</p> <ul> <li>Ingest from multiple sources</li> <li>Apply your Python package for transformations</li> <li>Implement bronze \u2192 silver \u2192 gold layers</li> <li>Add data quality checks</li> </ul> </li> <li> <p>Implement CI/CD:</p> <ul> <li>Automated testing on Git push</li> <li>Deploy through dev \u2192 test \u2192 prod</li> <li>Use Fabric deployment pipelines OR Azure DevOps OR GitHub Actions</li> </ul> </li> </ol>"},{"location":"day1/scenario/#phase-4-security-presentation-day-4-morning","title":"Phase 4: Security &amp; Presentation (Day 4 Morning)","text":"<ol> <li> <p>Implement security controls:</p> <ul> <li>Row-level security for branch-specific data</li> <li>Access controls for sensitive member data</li> <li>Document security practices</li> </ul> </li> <li> <p>Create monitoring and alerting</p> </li> <li> <p>Finalise documentation</p> </li> </ol>"},{"location":"day1/scenario/#phase-5-presentation-day-4-afternoon","title":"Phase 5: Presentation (Day 4 Afternoon)","text":"<ol> <li> <p>Present your solution:</p> <ul> <li>Business problem and requirements</li> <li>Architecture design and decisions</li> <li>Live demo of the working pipeline</li> <li>Code quality and testing approach</li> <li>CI/CD implementation</li> <li>Security implementation</li> <li>Challenges and learnings</li> </ul> </li> </ol>"},{"location":"day1/scenario/#provided-assets","title":"Provided Assets","text":"<p>You will receive:</p>"},{"location":"day1/scenario/#sample-datasets","title":"Sample datasets:","text":"<ul> <li><code>circulation_data.csv</code> (50,000 rows with quality issues)</li> <li><code>events_data.json</code> (API responses, nested structure)</li> <li><code>feedback.txt</code> (unstructured survey responses)</li> <li><code>catalogue.xlsx</code> (book metadata with formatting issues)</li> </ul>"},{"location":"day1/scenario/#other","title":"Other","text":"<ul> <li>Data dictionary describing expected schemas</li> <li>Business rules for validation</li> <li>Quality requirements (e.g., \"circulation data must have &lt; 1% missing ISBN\")</li> </ul>"},{"location":"day1/write-code/","title":"Write first Python code","text":""},{"location":"day1/write-code/#activity-first-code","title":"Activity: First Code","text":"<ul> <li>Open <code>src/data_processing/ingestion.py</code></li> <li>Implement <code>load_csv()</code> function</li> </ul> <p>Test it in Jupyter notebook:</p> <pre><code>import sys\nsys.path.append('/path/to/library-pipeline/src')\nfrom data_processing.ingestion import load_csv\n\ndf = load_csv('data/circulation_data.csv')\nprint(df.head())\nprint(df.info())\n</code></pre> <p>Commit and push</p>"},{"location":"day1/your-mission/","title":"Your Mission","text":""},{"location":"day1/your-mission/#your-mission","title":"Your Mission","text":"<p>This project uses sample data from our fictional library network.</p>"},{"location":"day1/your-mission/#your-task","title":"Your Task","text":"<p>Build a data quality pipeline that:</p> <ul> <li>\u2705 Ingests all data sources</li> <li>\u2705 Cleans and validates the data</li> <li>\u2705 Produces analysis-ready datasets</li> <li>\u2705 Handles errors gracefully</li> </ul>"},{"location":"day1/your-mission/#files","title":"Files","text":"<ul> <li><code>circulation_data.csv</code> - Book circulation transactions (5,000 rows)</li> <li><code>events_data.json</code> - Library events data (nested JSON)</li> <li><code>feedback.txt</code> - Unstructured member feedback</li> <li><code>catalogue.xlsx</code> - Book catalogue with metadata</li> </ul>"},{"location":"day1/your-mission/#data-quality-issues","title":"Data Quality Issues","text":"<p>These files intentionally contain data quality issues:</p> <ul> <li>Duplicate records</li> <li>Missing values</li> <li>Inconsistent date formats</li> <li>Nested structures requiring flattening</li> </ul> <p>Your task is to build a pipeline that cleans and validates this data.</p> <pre><code># In Jupyter notebook \nimport pandas as pd\n\n# Show the messiness\ndf = pd.read_csv('data/circulation_data.csv')\n\nprint(\"First few rows:\")\nprint(df.head())\n\nprint(\"\\nData info:\")\nprint(df.info())\n\nprint(\"\\nCheck for issues:\")\nprint(f\"Duplicates: {df.duplicated().sum()}\")\nprint(f\"Missing ISBNs: {df['isbn'].isna().sum()}\")\nprint(f\"Date types: {df['checkout_date'].dtype}\")\n\n# Show the problems!\nprint(\"\\nSample problematic rows:\")\nprint(df[df['isbn'].isna()].head())\n</code></pre>"},{"location":"day2/cleaning-enhancements/","title":"Write function - Data Cleaning","text":""},{"location":"day2/cleaning-enhancements/#activity-complete-and-improve-the-cleaning-function","title":"Activity: Complete and improve the Cleaning function","text":""},{"location":"day2/cleaning-enhancements/#task-1-build-cleaningpy","title":"Task 1: Build <code>cleaning.py</code>","text":"<p>Implement these functions:</p> <ol> <li><code>remove_duplicates()</code> (already shown)</li> <li><code>handle_missing_values()</code> (already shown)</li> <li><code>standardize_dates()</code> (already shown)</li> <li>Your own: <code>validate_isbn()</code> - check if ISBN is valid format</li> <li>Your own: <code>standardize_text()</code> - trim whitespace, lowercase, etc.</li> </ol>"},{"location":"day2/cleaning-enhancements/#task-2-test-in-jupyter","title":"Task 2: Test in Jupyter","text":"<p>Add another cell to the notebook you used earlier:</p> <pre><code>from data_processing.cleaning import (\n    remove_duplicates,\n    handle_missing_values,\n    standardize_dates\n)\n\n# Load data\ndf = load_csv('../data/circulation_data.csv')\n\n# Apply cleaning pipeline\ndf_clean = remove_duplicates(df, subset=['transaction_id'])\ndf_clean = handle_missing_values(df_clean, strategy='drop')\ndf_clean = standardize_dates(df_clean, ['checkout_date', 'return_date'])\n\n# Check results\nprint(f\"Original rows: {len(df)}\")\nprint(f\"Clean rows: {len(df_clean)}\")\nprint(df_clean.info())\n</code></pre>"},{"location":"day2/cleaning-enhancements/#task-2-commit-your-work","title":"Task 2: Commit Your Work","text":"<p>From the command line:</p> <pre><code>git add src/data_processing/cleaning.py\ngit commit -m \"Implement data cleaning functions\"\ngit push\n</code></pre> <p>Or commit using GitHub Desktop</p>"},{"location":"day2/cleaning-tests/","title":"Write unit tests","text":""},{"location":"day2/cleaning-tests/#activity-write-python-tests","title":"Activity: Write Python Tests","text":""},{"location":"day2/cleaning-tests/#task-1-write-tests-for-cleaning-module","title":"Task 1: Write tests for cleaning module","text":"<p>Create <code>tests/test_cleaning.py</code> with:</p> <ul> <li>At least 2 tests for <code>remove_duplicates()</code></li> <li>At least 2 tests for <code>handle_missing_values()</code></li> <li>At least 1 test for <code>standardize_dates()</code></li> <li>Use fixtures for test data</li> <li>Use <code>pandas.testing.assert_frame_equal()</code></li> </ul>"},{"location":"day2/cleaning-tests/#task-2-run-tests-and-check-coverage","title":"Task 2: Run tests and check coverage","text":"<pre><code># Run tests\npytest tests/test_cleaning.py -v\n\n# Check coverage\npytest tests/ --cov=src --cov-report=term-missing\n</code></pre> <p>Aim for 70%+ coverage!</p>"},{"location":"day2/cleaning/","title":"Explain function - Data Cleaning","text":""},{"location":"day2/cleaning/#data-cleaning-module","title":"Data Cleaning Module","text":""},{"location":"day2/cleaning/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Build reusable data cleaning functions</li> <li>Work with copies (avoid mutating inputs)</li> <li>Handle common data quality issues</li> <li>Apply cleaning to sample data</li> </ul>"},{"location":"day2/cleaning/#key-principle","title":"Key Principle","text":"<p>Always work on a COPY of the DataFrame. Never modify the input!</p>"},{"location":"day2/cleaning/#part-1-live-coding","title":"Part 1: Live Coding","text":"<p>Open <code>src/data_processing/cleaning.py</code> and add this code</p> <pre><code>\"\"\"Data cleaning functions for library pipeline.\n\nThis module contains functions for cleaning and standardizing data.\nAll functions return new DataFrames without modifying the input.\n\"\"\"\n\nimport pandas as pd\nimport logging\nfrom typing import List, Optional\n\nlogger = logging.getLogger(__name__)\n\ndef remove_duplicates(df, subset=None):\n    \"\"\"Remove duplicate rows from DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame\n        subset (list, optional): Columns to consider for duplicates\n\n    Returns:\n        pd.DataFrame: DataFrame with duplicates removed\n\n    Example:\n        &gt;&gt;&gt; df_clean = remove_duplicates(df, subset=['transaction_id'])\n    \"\"\"\n    df = df.copy()  # Work on a copy!\n\n    initial_rows = len(df)\n    df = df.drop_duplicates(subset=subset, keep='first')\n    removed = initial_rows - len(df)\n\n    if removed &gt; 0:\n        logger.info(f\"Removed {removed} duplicate rows\")\n\n    return df\n\ndef handle_missing_values(df, strategy='drop', fill_value=None, columns=None):\n    \"\"\"Handle missing values in DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame\n        strategy (str): 'drop', 'fill', or 'forward_fill'\n        fill_value: Value to fill if strategy='fill'\n        columns (list, optional): Specific columns to handle\n\n    Returns:\n        pd.DataFrame: DataFrame with missing values handled\n\n    Example:\n        &gt;&gt;&gt; df_clean = handle_missing_values(df, strategy='drop')\n        &gt;&gt;&gt; df_filled = handle_missing_values(df, strategy='fill', fill_value=0)\n    \"\"\"\n    df = df.copy()\n\n    if columns:\n        target_cols = columns\n    else:\n        target_cols = df.columns\n\n    initial_rows = len(df)\n\n    if strategy == 'drop':\n        df = df.dropna(subset=target_cols)\n        logger.info(f\"Dropped {initial_rows - len(df)} rows with missing values\")\n\n    elif strategy == 'fill':\n        if fill_value is None:\n            raise ValueError(\"fill_value must be provided when strategy='fill'\")\n        df[target_cols] = df[target_cols].fillna(fill_value)\n        logger.info(f\"Filled missing values with {fill_value}\")\n\n    elif strategy == 'forward_fill':\n        df[target_cols] = df[target_cols].fillna(method='ffill')\n        logger.info(\"Forward filled missing values\")\n\n    else:\n        raise ValueError(f\"Unknown strategy: {strategy}\")\n\n    return df\n\ndef standardize_dates(df, date_columns, date_format='%Y-%m-%d'):\n    \"\"\"Standardize date columns to consistent format.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame\n        date_columns (list): Column names containing dates\n        date_format (str): Target date format\n\n    Returns:\n        pd.DataFrame: DataFrame with standardized dates\n\n    Example:\n        &gt;&gt;&gt; df_clean = standardize_dates(df, ['checkout_date', 'return_date'])\n    \"\"\"\n    df = df.copy()\n\n    for col in date_columns:\n        if col not in df.columns:\n            logger.warning(f\"Column {col} not found in DataFrame\")\n            continue\n\n        try:\n            df[col] = pd.to_datetime(df[col], errors='coerce')\n            logger.info(f\"Standardized dates in column: {col}\")\n        except Exception as e:\n            logger.error(f\"Error standardizing dates in {col}: {e}\")\n            raise\n\n    return df\n</code></pre>"},{"location":"day2/cleaning/#key-points","title":"Key Points","text":"<ul> <li>Always <code>df = df.copy()</code> at start</li> <li>Log what you did</li> <li>Return new DataFrame</li> <li>Handle edge cases (column doesn't exist)</li> <li>Type hints in docstrings</li> </ul>"},{"location":"day2/coverage-goal/","title":"Achive test coverage","text":""},{"location":"day2/coverage-goal/#activity-achieve-high-test-coverage","title":"Activity: Achieve high Test coverage","text":""},{"location":"day2/coverage-goal/#task-1-test-ingestion-module","title":"Task 1: Test ingestion module","text":"<p>Create <code>tests/test_ingestion.py</code>:</p> <pre><code>\"\"\"Tests for data ingestion functions.\"\"\"\n\nimport pytest\nimport pandas as pd\nfrom pathlib import Path\nfrom src.data_processing.ingestion import load_csv, load_json, load_excel\n\n# Test with actual sample files\ndef test_load_csv_success():\n    \"\"\"Test loading real CSV file.\"\"\"\n    df = load_csv('data/circulation_data.csv')\n\n    assert len(df) &gt; 0\n    assert 'transaction_id' in df.columns\n\ndef test_load_csv_file_not_found():\n    \"\"\"Test error handling when file doesn't exist.\"\"\"\n    with pytest.raises(FileNotFoundError):\n        load_csv('data/nonexistent.csv')\n\ndef test_load_json_success():\n    \"\"\"Test loading real JSON file.\"\"\"\n    df = load_json('data/events_data.json')\n\n    assert len(df) &gt; 0\n    assert isinstance(df, pd.DataFrame)\n\n# Add more tests...\n</code></pre>"},{"location":"day2/coverage-goal/#task-2-run-coverage-report","title":"Task 2: Run coverage report","text":"<pre><code># Full coverage report\npytest tests/ -v --cov=src.data_processing --cov-report=term-missing\n\n# See which lines aren't covered\npytest tests/ --cov=src.data_processing --cov-report=html\n\n# Open htmlcov/index.html in browser\n</code></pre>"},{"location":"day2/coverage-goal/#task-3-improve-coverage","title":"Task 3: Improve coverage","text":"<ul> <li>Identify uncovered lines</li> <li>Write tests for those lines</li> <li>Focus on error handling and edge cases</li> </ul>"},{"location":"day2/coverage-goal/#task-4-document-and-commit","title":"Task 4: Document and commit","text":"<pre><code># Update README with coverage\necho \"Test Coverage: 75%\" &gt;&gt; README.md\n\ngit add tests/\ngit commit -m \"Add comprehensive tests - 75% coverage achieved\"\ngit push\n</code></pre> <p>Or make these changes in Visual Studio Code and GitHub Desktop</p>"},{"location":"day2/coverage/","title":"Python Test Coverage","text":""},{"location":"day2/coverage/#achieve-test-coverage","title":"Achieve Test Coverage","text":""},{"location":"day2/coverage/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Write comprehensive tests for ingestion module</li> <li>Reach 70%+ test coverage target</li> <li>Understand what good coverage means</li> <li>Fix any failing tests</li> </ul>"},{"location":"day2/coverage/#what-is-test-coverage","title":"What is Test Coverage?","text":"<pre><code>Coverage = (Lines of code executed by tests) / (Total lines of code)\n\nExample:\nYour code:     10 lines\nTests run:      7 lines\nCoverage:      70%\n</code></pre> <p>But: Coverage isn't everything!</p> <ul> <li>70% coverage with good tests &gt; 90% coverage with meaningless tests</li> <li>Test behavior, not just lines</li> <li>Edge cases matter more than happy path</li> </ul>"},{"location":"day2/coverage/#how-can-you-see-the-coverage","title":"How can you see the coverage?","text":""},{"location":"day2/coverage/#full-coverage-report","title":"Full coverage report","text":"<p><code>pytest tests/ -v --cov=src.data_processing --cov-report=term-missing</code></p>"},{"location":"day2/coverage/#see-which-lines-arent-covered","title":"See which lines aren't covered","text":"<p><code>pytest tests/ --cov=src.data_processing --cov-report=html</code></p> <p>Then open <code>htmlcov/index.html</code> in a browser.</p>"},{"location":"day2/coverage/#guidelines-for-this-project","title":"Guidelines for this project:","text":"<ul> <li>Stretch Goal: 80% coverage</li> <li>Acceptable: 70% coverage</li> <li>Minimum: All critical functions tested</li> </ul>"},{"location":"day2/ingestion-enhancements/","title":"Write function - Data Ingestion","text":""},{"location":"day2/ingestion-enhancements/#activity-complete-and-improve-the-ingestion-function","title":"Activity: Complete and improve the Ingestion function","text":""},{"location":"day2/ingestion-enhancements/#task-1-complete-ingestionpy","title":"Task 1: Complete <code>ingestion.py</code>","text":"<ul> <li>Implement or improve <code>load_csv()</code> and <code>load_json()</code></li> <li>Add <code>load_excel()</code> function for Excel files:</li> </ul> <pre><code>def load_excel(filepath, sheet_name=0, **kwargs):\n    \"\"\"Load Excel file into DataFrame.\"\"\"\n    # TODO: Implement this\n    pass\n</code></pre> <ul> <li>Test each function in Jupyter notebook</li> <li>Verify they work with sample data</li> </ul>"},{"location":"day2/ingestion-enhancements/#task-2-commit-your-work","title":"Task 2: Commit Your Work","text":"<p>From the command line:</p> <pre><code>git add src/data_processing/ingestion.py\ngit commit -m \"Implement data ingestion functions for CSV, JSON, and Excel\"\ngit push origin main\n</code></pre> <p>Or commit using GitHub Desktop</p>"},{"location":"day2/ingestion/","title":"Explain function - Data Ingestion","text":""},{"location":"day2/ingestion/#data-ingestion-module","title":"Data Ingestion Module","text":""},{"location":"day2/ingestion/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Build functions to load CSV, JSON, and Excel files</li> <li>Implement proper error handling and logging</li> <li>Write clean, documented code</li> <li>Test functions locally in Jupyter</li> </ul>"},{"location":"day2/ingestion/#principles","title":"Principles","text":"<p>We're building a Python package, not just scripts. This means:</p> <ul> <li>Functions do ONE thing well</li> <li>Docstrings for every function</li> <li>Error handling (what if file doesn't exist?)</li> <li>Logging (so we can debug later)</li> <li>Pure functions (don't modify inputs)</li> </ul>"},{"location":"day2/ingestion/#part-1-live-coding","title":"Part 1: Live Coding","text":"<p>Open <code>src/data_processing/ingestion.py</code> and add this code:</p> <pre><code>\"\"\"Data ingestion functions for library pipeline.\n\nThis module handles loading data from various file formats.\n\"\"\"\n\nimport pandas as pd\nimport json\nimport logging\nfrom pathlib import Path\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef load_csv(filepath, **kwargs):\n    \"\"\"Load CSV file into DataFrame.\n\n    Args:\n        filepath (str): Path to CSV file\n        **kwargs: Additional arguments for pd.read_csv()\n\n    Returns:\n        pd.DataFrame: Loaded data\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        pd.errors.EmptyDataError: If file is empty\n\n    Example:\n        &gt;&gt;&gt; df = load_csv('data/circulation.csv')\n        &gt;&gt;&gt; print(len(df))\n    \"\"\"\n    filepath = Path(filepath)\n\n    # Check file exists\n    if not filepath.exists():\n        logger.error(f\"File not found: {filepath}\")\n        raise FileNotFoundError(f\"File not found: {filepath}\")\n\n    try:\n        logger.info(f\"Loading CSV from {filepath}\")\n        df = pd.read_csv(filepath, **kwargs)\n        logger.info(f\"Successfully loaded {len(df)} rows from {filepath}\")\n        return df\n\n    except pd.errors.EmptyDataError:\n        logger.error(f\"CSV file is empty: {filepath}\")\n        raise\n    except Exception as e:\n        logger.error(f\"Error loading CSV {filepath}: {e}\")\n        raise\n\ndef load_json(filepath):\n    \"\"\"Load JSON file and flatten to DataFrame.\n\n    Args:\n        filepath (str): Path to JSON file\n\n    Returns:\n        pd.DataFrame: Flattened data\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        json.JSONDecodeError: If file is not valid JSON\n\n    Example:\n        &gt;&gt;&gt; df = load_json('data/events.json')\n    \"\"\"\n    filepath = Path(filepath)\n\n    if not filepath.exists():\n        logger.error(f\"File not found: {filepath}\")\n        raise FileNotFoundError(f\"File not found: {filepath}\")\n\n    try:\n        logger.info(f\"Loading JSON from {filepath}\")\n        with open(filepath, 'r') as f:\n            data = json.load(f)\n\n        # Flatten nested structure\n        # Adjust based on your JSON structure\n        if isinstance(data, dict) and 'events' in data:\n            df = pd.json_normalize(data['events'])\n        else:\n            df = pd.json_normalize(data)\n\n        logger.info(f\"Successfully loaded {len(df)} records from {filepath}\")\n        return df\n\n    except json.JSONDecodeError as e:\n        logger.error(f\"Invalid JSON in {filepath}: {e}\")\n        raise\n    except Exception as e:\n        logger.error(f\"Error loading JSON {filepath}: {e}\")\n        raise\n</code></pre>"},{"location":"day2/ingestion/#key-points-to-note","title":"Key Points to note:","text":"<ul> <li>Type hints in docstrings</li> <li>Error handling with try/except</li> <li>Logging at INFO and ERROR levels</li> <li>Raising exceptions (don't hide errors!)</li> <li>Docstring format (Google style)</li> </ul>"},{"location":"day2/ingestion/#part-2-test-in-jupyter","title":"Part 2: Test in Jupyter","text":"<ul> <li>Create a <code>notebooks</code> folder</li> <li>Create a new Jupyter Notebook in this folder</li> <li>Copy and run the following code:</li> </ul> <pre><code># In Jupyter notebook\nimport sys\nsys.path.append('C:\\\\Users\\\\Admin\\\\Documents\\\\GitHub\\\\library-pipeline\\\\src')\n\nfrom data_processing.ingestion import load_csv, load_json\n\n# Test CSV loading\ndf_circ = load_csv('../data/circulation_data.csv')\nprint(f\"Loaded {len(df_circ)} circulation records\")\nprint(df_circ.head())\n\n# Test JSON loading\ndf_events = load_json('../data/events_data.json')\nprint(f\"Loaded {len(df_events)} events\")\nprint(df_events.head())\n</code></pre>"},{"location":"day2/python-testing/","title":"Python Testing","text":""},{"location":"day2/python-testing/#python-testing","title":"Python Testing","text":""},{"location":"day2/python-testing/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand why we test data code</li> <li>Learn pytest basics and fixtures</li> <li>Use <code>pandas.testing.assert_frame_equal()</code></li> <li>Write first tests for cleaning module</li> </ul>"},{"location":"day2/python-testing/#part-1-why-test","title":"Part 1: Why Test?","text":"<pre><code># What if someone changes your function?\ndef remove_duplicates(df):\n    return df.drop_duplicates()  # \u274c But what if it's wrong?\n</code></pre> <p>Without tests: You won't know until production breaks!</p> <p>With tests: Tests fail immediately!</p>"},{"location":"day2/python-testing/#part-2-the-pandas-testing-problem","title":"Part 2: The Pandas Testing Problem","text":"<p>Run this in a Jupyter Notebook:</p> <pre><code>import pandas as pd\n\nresult = pd.DataFrame({'col': [1, 2, 3]})\nexpected = pd.DataFrame({'col': [1, 2, 3]})\n\n# Try the wrong way\nprint(\"result == expected:\")\nprint(result == expected)  # DataFrame of bools!\n\n# Try to assert (this will fail)\ntry:\n    assert result == expected\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"day2/python-testing/#the-solution","title":"The solution","text":"<pre><code>import pandas.testing as pdt\n\n# The right way\npdt.assert_frame_equal(result, expected)\nprint(\"\u2705 Test passed!\")\n</code></pre>"},{"location":"day2/python-testing/#part-3-writing-tests","title":"Part 3: Writing Tests","text":"<p>Create <code>tests/test_cleaning.py</code>:</p> <pre><code>\"\"\"Tests for data cleaning functions.\n\nThis module demonstrates proper testing patterns for Pandas code.\n\"\"\"\n\nimport pytest\nimport pandas as pd\nimport pandas.testing as pdt\nfrom src.data_processing.cleaning import (\n    remove_duplicates,\n    handle_missing_values,\n    standardize_dates\n)\n\n# ========================================\n# FIXTURES - Reusable test data\n# ========================================\n\n@pytest.fixture\ndef sample_df_with_duplicates():\n    \"\"\"Sample DataFrame with duplicate rows.\"\"\"\n    return pd.DataFrame({\n        'id': [1, 2, 2, 3, 3, 3],\n        'name': ['Alice', 'Bob', 'Bob', 'Charlie', 'Charlie', 'Charlie'],\n        'value': [10, 20, 20, 30, 30, 30]\n    })\n\n@pytest.fixture\ndef sample_df_with_missing():\n    \"\"\"Sample DataFrame with missing values.\"\"\"\n    return pd.DataFrame({\n        'id': [1, 2, 3, 4],\n        'name': ['Alice', None, 'Charlie', 'David'],\n        'value': [10, 20, None, 40]\n    })\n\n# ========================================\n# TESTS FOR remove_duplicates()\n# ========================================\n\ndef test_remove_duplicates_exact(sample_df_with_duplicates):\n    \"\"\"Test duplicate removal using exact DataFrame comparison.\"\"\"\n    result = remove_duplicates(sample_df_with_duplicates, subset=['id'])\n\n    expected = pd.DataFrame({\n        'id': [1, 2, 3],\n        'name': ['Alice', 'Bob', 'Charlie'],\n        'value': [10, 20, 30]\n    })\n\n    # Reset index for comparison\n    result = result.reset_index(drop=True)\n\n    pdt.assert_frame_equal(result, expected)\n\ndef test_remove_duplicates_properties(sample_df_with_duplicates):\n    \"\"\"Test duplicate removal using property assertions.\"\"\"\n    result = remove_duplicates(sample_df_with_duplicates, subset=['id'])\n\n    # Test properties instead of exact values\n    assert len(result) == 3\n    assert result['id'].is_unique\n    assert set(result['id']) == {1, 2, 3}\n\ndef test_remove_duplicates_no_changes():\n    \"\"\"Test with DataFrame that has no duplicates.\"\"\"\n    df_unique = pd.DataFrame({\n        'id': [1, 2, 3],\n        'name': ['A', 'B', 'C']\n    })\n\n    result = remove_duplicates(df_unique, subset=['id'])\n\n    pdt.assert_frame_equal(result, df_unique)\n\ndef test_remove_duplicates_empty():\n    \"\"\"Test with empty DataFrame.\"\"\"\n    empty_df = pd.DataFrame({'id': [], 'name': []})\n    result = remove_duplicates(empty_df)\n\n    assert len(result) == 0\n    pdt.assert_frame_equal(result, empty_df)\n\n# ========================================\n# TESTS FOR handle_missing_values()\n# ========================================\n\ndef test_handle_missing_drop(sample_df_with_missing):\n    \"\"\"Test dropping rows with missing values.\"\"\"\n    result = handle_missing_values(sample_df_with_missing, strategy='drop')\n\n    # Should only have rows without any NaN\n    assert len(result) == 2\n    assert result['name'].notna().all()\n    assert result['value'].notna().all()\n\ndef test_handle_missing_fill(sample_df_with_missing):\n    \"\"\"Test filling missing values.\"\"\"\n    result = handle_missing_values(\n        sample_df_with_missing, \n        strategy='fill', \n        fill_value=0\n    )\n\n    # Should have all 4 rows\n    assert len(result) == 4\n    # No missing values\n    assert result['name'].notna().all() or (result['name'] == 0).any()\n    assert result['value'].notna().all()\n\ndef test_handle_missing_invalid_strategy(sample_df_with_missing):\n    \"\"\"Test that invalid strategy raises error.\"\"\"\n    with pytest.raises(ValueError, match=\"Unknown strategy\"):\n        handle_missing_values(sample_df_with_missing, strategy='invalid')\n</code></pre> <p>Run the tests: </p><pre><code>cd C:\\Users\\Admin\\Documents\\GitHub\\library-pipeline\n\n# Run all tests\npytest tests/test_cleaning.py -v\n\n# Run with coverage\npytest tests/test_cleaning.py -v --cov=src.data_processing.cleaning --cov-report=term-missing\n</code></pre><p></p>"},{"location":"day2/python-testing/#key-points","title":"Key Points","text":"<ul> <li>Use <code>@pytest.fixture</code> for reusable test data</li> <li>Use <code>pdt.assert_frame_equal()</code> for exact comparison</li> <li>Test properties when exact comparison is too strict</li> <li>Test edge cases (empty, no changes needed)</li> <li>Test error cases with <code>pytest.raises()</code></li> <li>Descriptive test names: <code>test_&lt;function&gt;_&lt;scenario&gt;</code></li> </ul>"},{"location":"day2/vm-setup/","title":"VM Setup & Welcome","text":""},{"location":"day2/vm-setup/#setup","title":"Setup","text":""},{"location":"day2/vm-setup/#step-1-create-a-branch-in-github","title":"Step 1: Create a branch in GitHub","text":"<ul> <li>In GitHub - go to the repository you created yesterday</li> <li>Click <code>main</code></li> <li>Enter <code>day2</code> in the box: Find or create a branch</li> <li>Click: Create branch day2 from main</li> </ul>"},{"location":"day2/vm-setup/#step-2-set-usernme-and-email-in-vm-for-git","title":"Step 2: Set usernme and email in VM for Git","text":"<p>At a prompt in the VM copy and paste the following 2 lines:</p> <pre><code>git config --global user.email \"you@example.com\"\ngit config --global user.name \"Your Name\"\n</code></pre>"},{"location":"day2/vm-setup/#step-3-in-the-vm-use-github-desktop-to-clone-your-repo","title":"Step 3: In the VM use GitHub Desktop to clone your repo","text":"<ul> <li>Open GitHub Desktop in the Virtual Machine</li> <li>Click: <code>Clone a repository from the internet...</code></li> <li>Click the button: <code>GitHub.com</code></li> <li>Click the button: <code>Sign in</code></li> <li>Click: <code>Continue with browser</code></li> <li>Enter your Username and Password (and 2FA if prompted)</li> </ul> <p>Find the repo you created yesterday from the list and click it</p> <ul> <li>Click: <code>Clone</code></li> <li>Click the dropdown: <code>Current branch</code></li> <li>Select: <code>origin/day2</code></li> </ul>"},{"location":"day2/vm-setup/#step-4-create-your-virtual-enviroment","title":"Step 4: Create your virtual enviroment","text":"<p>Make sure you are at a prompt in your repo root (<code>library-pipeline</code>)</p> <p>Run the following:</p> <pre><code>python -m venv venv\n\n.\\venv\\Scripts\\Activate.ps1\n\npip install -r requirements.txt\n\npytest tests/ -v\n</code></pre>"},{"location":"day2/vm-setup/#step-5-open-your-repo-in-visual-studio-code","title":"Step 5: Open your repo in Visual Studio Code","text":"<ul> <li>In GitHub Desktop ~ Click the button: <code>Open in Visual Studio Code</code></li> </ul>"},{"location":"day3/","title":"Index","text":""},{"location":"day4/","title":"Index","text":""}]}